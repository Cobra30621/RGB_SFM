\documentclass[class=NCU_thesis, crop=false]{standalone}
\begin{document}

\chapter{緒論}
\section{研究動機}

自1998年LeNet\cite{726791}問世以來，隨著深度學習的蓬勃發展，人工智慧應用範圍也逐步融入到人們日常生活的方方面面。
儘管人工智慧的發展如此蓬勃，實際上我們對於人工智慧的實際運作過程與做出決策的理由仍然存在著許多未知的地方。
目前，大部分模型均為"black-box"(黑盒)模型，我們雖了解其運作理論，但卻無法得知其每個決策的具體理由和依據。

當人工智慧開始運用到各行各業時，人們開始發覺在某些領域或是應用情境(如：醫療、軍事、金融等)下，
這些領域所需要的決策往往需要合理的理由或是因果關係的推論支撐才足以讓使用者有足夠的信心採用，
在此情況下，具備可解釋性的深度學習模型做出令使用者有信心採用的決策。

隨著美國國防部MAPPA在2016年將可解釋性人工智慧（Explainable Artificial Intelligence, XAI）列為Third-wave AI systems
並加入\textit{Defense Advanced Research Projects Agency(DARPA)} \cite{DARPA}、
歐盟也在同年通過了\textit{European Union's General Data Protection Regulation (GDPR)}裡面規範使用者有獲得有關於推論資訊的 
"Meaningful information about the logic involved" 的權利\cite{GDPR2016a}, \cite{doi:10.1080/13600834.2019.1573501}。 這些重要的政策舉措使得可解釋性的深度學習模型成為了全球範圍內的熱門研究，不僅在學術界，也在企業界甚至國家層面都被視為重要的發展項目。

\pagebreak
\section{研究目的}
本論文旨在深入研究2023年由J.F Yang 等人所提出模擬皮層多層構造的可解釋性模型 CNN-based Interpretable Model(以下簡稱CIM) \cite{YangCNNInterpretable}，
在此基礎上進行效能改進並進一步開發出一個不只適用於灰階影像而能更廣泛的適用於RGB彩色影像之可解釋性模型，
使其在保持原來CIM模型的高準確度與高可解釋性的水準下可以應用於更多現實影像分類任務。

透過研究人眼如何辨識彩色影像，
我們希望設計出用於模擬人眼感知色彩機構的色彩感知區塊和感知輪廓的輪廓感知區塊，
使模型可以模仿人眼感知外部資訊的過程，並將兩者資訊分別輸入特徵傳遞區塊。
藉由卷積模組、響應過濾模組、空間合併模組，模擬人腦多層皮質資訊傳遞，
每一層的空間合併模組都會將輸入的特徵在保留空間關係的前提下進行合併，
最終形成較為完整的特徵資訊並輸入全連接層以學習每個分類的特徵。

此外本論文也希望開發出來的適用於新型可解釋性模型能夠針對每一層的輸出之特徵進行分析，
並找出各層輸出特徵與最後預測分類之間的關係，以理解該模型是根據何種特徵做出分類判斷，從而形成一個使用者可以接受之解釋。

\pagebreak
\section{論文架構}

本論文分為五個章節，架構如下:

第一章：緒論，敘述本論文的研究動機、目的和架構

第二章：背景知識與文件回顧，介紹本論文所需之背景知識與回顧可解釋性人工智慧的演進與各個分類的重要論文

第三章：研究方法，介紹本論文對以卷積神經網路為基礎之新型可解釋性深度學習模型的架構與方法

第四章：實驗設計與結果，對本論文所提出的方法在不同資料集上的效果進行實驗與觀察

第五章：總結，對本論文之結果做出結論並提出未來可行之研究方向


\end{document}

