\documentclass[class=NCU_thesis, crop=false]{standalone}
\begin{document}

\chapter{緒論}
\section{研究動機}

自1998年LeNet\cite{726791}問世以來，隨著深度學習的蓬勃發展，人工智慧應用範圍也逐步融入到人們日常生活的方方面面。
然而儘管人工智慧的發展如此蓬勃，實際上我們對於人工智慧的實際運作過程與做出決策的理由仍然存在著許多未知的地方，
目前，大部分模型彷彿是一個黑盒子，我們雖了解其運作理論，但卻無法得知其每個決策的具體理由和依據。

當人工智慧開始運用到各行各業時，人們開始發覺在某些領域或是應用情境(如：醫療決策、軍事領域、金融決策等)下，
單單只有高準確度是無法讓使用者具備足夠的信心採用人工智慧所預測的決策，
這些領域所需要的決策往往需要合理的理由或是因果關係的推論支撐才足以讓使用者有足夠的信心採用，
在此情況下，具備可解釋性的深度學習模型做出令使用者有信心採用的決策。

隨著美國國防部MAPPA在2016年將可解釋性人工智慧（XAI）列為third-wave AI systems列為DARPA計畫項目之一\cite{DARPA}、
歐盟也在同年通過了《 European Union's General Data Protection Regulation (GDPR) 》裡面規範使用者有獲得有關於推論資訊的 "meaningful information about the logic involved" 的權利\cite{GDPR2016a},\cite{doi:10.1080/13600834.2019.1573501}。 這些重要的政策舉措使得可解釋性的深度學習模型成為了全球範圍內的熱門研究，不僅在學術界，也在企業界甚至國家層面都被視為重要的發展項目。

\section{研究目的}



\section{論文架構}


\end{document}

