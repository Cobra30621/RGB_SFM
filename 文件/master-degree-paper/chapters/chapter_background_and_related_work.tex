\documentclass[class=NCU_thesis, crop=false]{standalone}
\begin{document}

\chapter{背景知識與文獻回顧}
\section{背景知識}

本章節將會介紹本論文所需的背景知識，可以幫助讀者更好地理解本論文提出的論文的概念和出發點，
內容包含：人如何感知彩色影像、大腦皮質的運作、卷積神經網路與以卷積神經網路為基礎的可解釋性深度學習模型。

\subsection{人如何感知彩色影像}

要了解人如何感知色彩我們必須要先了解將彩色影像的這條 Central Visual Pathway 會經過哪些的部位與流程，
根據《 Neuroscience 》 \cite{Purves2004Neuroscience3E}所介紹，
彩色影像在 Central Visual Pathway 會經過的部位總共可以分為三個重要部位：視網膜(Retina)、外側膝狀體(外膝體，Lateral Geniculate Nucleus)、視覺皮層(Visual Cortex)，
彩色影像從視網膜進入後會送入外膝體，外膝體在收到兩側眼球的資訊後會將不同的資訊平行傳輸至不同的視覺皮層，視覺皮層則負責對這些資訊進行分層的整合與感知。
詳細的Central Visual Pathway 如\cref{fig:Central_Visual_Pathway}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Visual_Pathway/Central_Visual_Pathway.jpg}
  \caption{詳細視覺路徑圖~\cite{Purves2004Neuroscience3E}}
  \label{fig:Central_Visual_Pathway}
\end{figure}

\subsubsection{視網膜}

外界事物將光反射進入眼睛當中，透過眼珠中的角膜與水晶體等透明的光學介質進行折射最終聚焦於視網膜表面的感光層上形成影像。
在光聚焦於視網膜表層時，視網膜會將光轉換成動作電位並透過視神經傳輸至外膝體，也因此Central Visual Pathway 才會將視網膜視為感知彩色影像的第一個重要部位。

根據《Neuroscience-Exploring the Brain》\cite{bear2016neuroscience}
我們知道視網膜的基礎架構是由五類細胞組成如\cref{fig:RetinaBasic}，分別是: 感光細胞(Photoreceptor cell)、 水平細胞(Horizontal cell)、 無長突細胞(Amacrine cell)、 雙極細胞(Bipolar cell)、 神經節細胞(Ganglion cell)。 感光細胞包含我們常聽到的視錐細胞等負責將輸入的光轉化為動作電位、 雙極細胞負責將會將感光細胞的電位傳送到神經節細胞、
這個傳輸的過程有些則是由水平細胞和無長突細胞協助傳輸，
神經節細胞則負責將最後的資訊傳輸到外膝體之中。

\begin{figure}[H]
  \centering
  \subcaptionbox{
    視網膜基本架構~\cite{bear2016neuroscience}
    \label{fig:RetinaBasic}}
    {\includegraphics[width=0.5\textwidth]{Visual_Pathway/RetinaBasic.png}
    }
~
  \subcaptionbox{
    視網膜層級架構~\cite{bear2016neuroscience}
    \label{fig:RetinaStructure}
  }
    {\includegraphics[width=0.9\textwidth]{Visual_Pathway/RetinaStructure.png}
    }
  \caption{視網膜整體架構}
  \label{fig:RetinaTotalStructure}
\end{figure}

上面講的基礎架構只是最基本的情況，事實上視網膜的各類細胞遠比上面要架構要複雜許多，上述的五大類細胞還可以再更細的分為不同功能的變種細胞，由此組成了極度複雜的視網膜層級系統如\cref{fig:RetinaStructure}。
如此複雜的視網膜系統，其功能當然不只負責影像的感知與電位傳輸，事實上
在2013年的\cite{annurev}，就已經發現在感光細胞將光轉換為動作電位後會將電位傳輸到層級架構的 Inner Plexiform Layer (IPL)中不同變種的雙極細胞，這些不同變種的雙極細胞對收到的影像資訊進行不同的平行處理，最終輸出影像中不同的方面的要素到神經節細胞，例如: 紅藍綠不同色彩、明暗的變化、影像輪廓等等， 這同時也是本論文在設計彩色影像感知時的核心概念。

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Visual_Pathway/IPL.jpg}
  \caption{視網膜對影像進行不同的平行處理~\cite{annurev}}
  \label{fig:IPL}
\end{figure}
\pagebreak

此外，在Jeff Hawkins的著作\cite{10.5555/993636}和《Principles of Neural Science》\cite{1180370208}中均有提到，
視網膜能接收到的影像資訊不只是上述的空間性資訊，
同時也具有不同時間性的資訊，這代表同樣的影像進入視網膜的型態是會隨時間而改變。
具體而言，眼睛會在每一秒鐘快速移動視線的焦點三到四次(如\cref{fig:Saccade})，
但人類的認知上不會有所感覺，這個行為被稱為 「 眼球跳動 」(Saccade)。
眼球跳動使得同一個影像產生時間上的變化，形成時間性的影像資訊。
這個概念也被運用於本論文的空間位置保留機制中的時間遺忘參數上，
使得影像特徵可以呈現時序上的不同。

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Visual_Pathway/Saccade.png}
  \caption{眼球跳動示意圖~\cite{annurev}}
  \label{fig:Saccade}
\end{figure}

\subsubsection{外側膝狀體}

外側膝狀體(外膝體)主要負責將視網膜不同的方面資訊(如:色彩、輪廓、運動方向…)傳輸到對應的初級視覺皮質，其中的細胞分層排列，每層分布排列著著不同種類的細胞。
除此之外，外膝體中不同的細胞層也對應著不同視野的半個視網膜形成如 \cref{fig:LGN_Relation}
的對應關係，這也表示視網膜中相鄰區域的同種類的影像資訊在外膝體中很可能在同一個細胞層，
這個性質也保證了外膝體在資訊傳輸的過程可以保留資訊的空間位置資訊。
由於視覺皮層會將影像資訊從初級到高級逐漸進行資訊整合和學習，
因此外膝體的存在可以協助視覺皮層將不同的影像資訊傳輸到對應的視覺皮質層，
這對視覺皮質能夠進行平行處理與整合起到至關重要的作用。

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Visual_Pathway/LGN_relation.png}
  \caption{視網膜與外膝體的對應關係~\cite{bear2016neuroscience}}
  \label{fig:LGN_Relation}
\end{figure}
\pagebreak

\subsubsection{視覺皮層}
皮層可以說是大腦最重要的地方，高級認知功能例如:記憶、思考、感覺等均在此發生。
根據\cite{1180370208}中的說明，人類的大腦中皮質以分層結構存在的，
而視覺皮層又可以分為初級視覺皮層(Primary visual cortex, V1)和紋外皮層(V2、V4、MT層)這四層。
初級視覺皮層負責接收外膝體傳輸來的資訊並開始處理顏色、方向、輪廓等視覺資訊，
其餘皮層則負責整合和傳遞資訊給下面的皮層，因此隨著皮層的深入，所得到的視覺資訊也會越來越完整。

\cite{1180370208}提出一種影像資訊處理架構，
示意圖如\cref{fig:ParallelProcess}，
其將皮質中的資訊傳輸路徑分成兩類，Ventral Pathway 和 Dorsal Pathway, 
Ventral Pathway 由 V1、V2、V4 組成負責處理影像的色彩、形狀等資訊，
而Dorsal Pathway 由 V1、V2、MT 組成負責處理影像的運動方向的資訊。
% 但同時也強調這個分類只是一個大致分類，
% 實際上不同的視覺特徵資訊(如顏色、運動方向等)在皮層中也會相互聯繫，
% 也就是存在所謂的側向連結，最終才能形成統一的影像感知。

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{Visual_Pathway/Parallel_processing_In_contex.png}
  \caption{Visual Pathway的認知過程~\cite{1180370208}}
  \label{fig:ParallelProcess}
\end{figure}

% 在皮層中，V1 將相同功能的神經束聚在一起形成不同功能的柱狀結構(例如顏色柱、方向柱、線條柱等)
% 這些柱狀體負責處理各自的視覺資訊並按照固定規則排列成層，
% 並且將左右眼的柱狀層交錯排列形成一個皮層計算模組(Cortical Computational Module)，如\cref{fig:CorticalComputationalModule}，重複上百次來覆蓋左右眼看到的所有影像。
% 這樣設計的目的是希望不管從V1中的任何位置和方向移動都可以漸進的對每塊視野的顏色、輪廓、方向、深度等特徵資訊進行充分的分析。
% 以上的兩個概念被本論文應用在色彩與輪廓特徵學習中，使我們的模型可以也可以充分學習色彩與輪廓兩種特徵，並隨著層數的深入逐漸組成更完整的影像特徵資訊。

% \begin{figure}[H]
%   \centering
%   \includegraphics[width=0.7\textwidth]{Visual_Pathway/cortical_computational_module.png}
%   \caption{皮層計算模組~\cite{1180370208}}
%   \label{fig:CorticalComputationalModule}
% \end{figure}
% \pagebreak

\pagebreak
根據\cite{10.5555993636}所述，他們透過實驗發現，
任何高階的皮層都同時接收著複數底層的皮層區所送上來的輸入，
因此，V1，V2，V3，MT不應該被視為單一皮層，
而是由許多個小皮質區所組成。
這些小皮質區不需要了解其所接收到的輸入的意義，
而其主要工作在於找出輸入之間的關係並記憶它們空間上之順序，
再利用這些資訊去計算其在未來的輸出。
這個概念也被運用在CIM和本模型的空間合併模組之中，
紀錄輸入的空間性關係並傳遞到下一層，
使下一層的高斯卷積能在有空間性資訊的輸入情況下進行預測。

\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{Visual_Pathway/cortical-hierarchy.png}
  \caption{皮層層狀架構~\cite{10.5555993636}}
  \label{fig:CorticalHierarchy}
\end{figure}
\pagebreak


\subsection{CNN-based Interpretable Model}
CNN-based Interpretable Model(CIM)\cite{YangCNNInterpretable}為 Yang 等人於2023年提出的可解釋性模型，
該模型是模擬大腦視覺皮層的階層架構和影像的空間性關係來解釋深度學習的決策的過程，
目的是希望使模型的決策過程透明化解決黑箱決策的問題。
然而該模型目前只能運用於灰階影像上而無法處理彩色影像的問題，
本論文的目標是基於此模型進行改進，開發出一個更適用於現實彩色影像的新模型。

\subsubsection{模型架構}
該模型採用多層結構，
每層由三個部分組成: 高斯卷積模組、特徵增強模組、空間位置保留合併模組，
整體模型架構如\cref{fig:CIM_arch}，
\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{CIM/CIM_model_arch.jpg}
  \caption{CIM架構圖~\cite{YangCNNInterpretable}}
  \label{fig:CIM_arch}
\end{figure}


高斯卷積模組使用高斯函數取代原始卷積操作中的內積操作來對輸入進行特徵提取，
使得該模組計算的結果具有相似度的意義並且此特性也被運用在後續的可解釋性中。\\

特徵增強模組使用該論文中新提出變形的ReLU函數稱為changed ReLU(簡稱為cReLU)，
來過濾不重要的特徵使得後續的解釋性可以有更好的發揮。
cReLU公式於\cref{eq:eq-crelu}，$c$為人工取的一個閥值，
\begin{equation}
    \label{eq:eq-crelu}
    % f(x) = \max(c, x)
    f(x)= 
    \begin{cases}
        0 & \text{if  $x < c$ }\\
        x & \text{if  $x \geq c$}
    \end{cases}
\end{equation}

空間位置保留合併模組為該論文所提出來的全新機制，
文中他們將一組輸入在經過高斯卷積和特徵增強後產生的輸出稱為$RM$,
而一張影像又擁有多組資訊，從而產生出多張屬於這個影像的$RM$。
合併公式如\cref{eq:eq-sf}，$RM_{c}$為合併後的RM，$RM_{k}$為第k張RM，$n$為輸入資訊的數量，$\alpha$為一個可訓練的時間遺忘函數(forgetting factor)。
空間位置保留合併模組使用此合併公式來模擬特徵資訊在皮層中的時序性特徵資訊與皮層的逐層合併的現象，
從而達到在合併的過程中保留特徵之間的時序關係,
也可讓越後面的層數學習到更完整的特徵資訊。
\begin{equation}
    \label{eq:eq-sf}
    RM_{c}=\frac{1}{n} \sum_{k = 0}^{n-1} \alpha^{k} \times RM_{k},  \qquad \ where\ n = \textit{H}^{i}_{SF} \times \textit{W}^{i}_{SF}
\end{equation}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{CIM/CIM_SFM.png}
  \caption{合併方式示意圖~\cite{YangCNNInterpretable}}
  \label{fig:CIM_SFM}
\end{figure}


\subsubsection{可解釋性}
文中將每層的高斯卷積模組的濾波器視為二階矩陣稱為特徵映射圖(Feature Map, $FM$),
一組輸入在每層經過高斯卷積模組和特徵增強模組後輸出進行維度轉換後的矩陣稱為特徵映射響應圖(Response Map, $RM$),
當模型訓練完畢後會讓每個卷積模組中的濾波器對資料集中的所有影像進行反應
並且從所有反應中找到造成最大反應的影像，每個濾波器都會有屬於自己的最大反映影像，
稱為特徵映射圖之對應影像(Corresponding Image, $CI$)
以上的$FM$, $RM$, $CI$ 便是模型提供可解釋性的核心要素，
其關係圖如\cref{fig:one-FM2-RM2-CI2-EXAMPLE}

\begin{figure}[H]
    %\captionsetup[subfigure]{labelformat=empty} % 完全隱藏圖號
    \centering
    \subcaptionbox
        {RM$_{0}$之特徵圖
        \label{fig:one-RM2-EXAMPLE}}
        {\includegraphics[width=0.33\linewidth]{CIM/fig-one-RM2-EXAMPLE.png}}
    ~
    \subcaptionbox
        {FM$_{0}$之特徵圖
        \label{fig:one-FM2-EXAMPLE}}
        {\includegraphics[width=0.33\linewidth]{CIM/fig-one-FM2-EXAMPLE.png}}
    ~
    \subcaptionbox
        {CI$_{0}$之特徵圖
        \label{fig:one-CI2-EXAMPLE}}
        {\includegraphics[width=0.475\linewidth]{CIM/fig-one-CI2-EXAMPLE.png}}
    \caption{FM-RM-CI\cite{YangCNNInterpretable}}
    \label{fig:one-FM2-RM2-CI2-EXAMPLE}
\end{figure}

在可解釋性的方法上，論文中提出了兩個方法，
第一個方法為特徵圖對應法，
在將一張影像輸入進模型後，
第一步視覺化輸入影像中每個區塊在模型中每層的$RM$並找出$RM$中最大反應，
第二步從$RM$最大反應找出造成反應的$FM$，
第三步根據$FM$和$CI$的對應關係，找出該$FM$對應之$CI$，
第四步利用每個區塊的$CI$根據原本的位置組合出輸入影像在模型中被分解的過程，
這個方法用於了解模型在經過高斯卷積、特徵增強、空間位置保留合併後所關注的特徵資訊。
範例如\cref{tab:CIM_RMCI_example}

\begin{table}[H]
  \centering
  \begin{tabular}{| c | c | c | c | c |}
    \hline
    原始影像 & RM-CI1 & RM-CI2 & RM-CI3 & RM-CI4 \\
    \hline
    \begin{minipage}[t]{0.18\columnwidth}\centering\includegraphics[width=0.9\textwidth]{CIM/mnist-16-0-cmp-input.png}\end{minipage} &
    \begin{minipage}[t]{0.18\columnwidth}\centering\includegraphics[width=0.9\textwidth]{CIM/mnist-16-0-cmp-rm1-ci-max-1.png}\end{minipage} &
    \begin{minipage}[t]{0.18\columnwidth}\centering\includegraphics[width=0.9\textwidth]{CIM/mnist-16-0-cmp-rm2-ci-max-1.png}\end{minipage} &
    \begin{minipage}[t]{0.18\columnwidth}\centering\includegraphics[width=0.9\textwidth]{CIM/mnist-16-0-cmp-rm3-ci-max-1.png}\end{minipage} &
    \begin{minipage}[t]{0.18\columnwidth}\centering\includegraphics[width=0.9\textwidth]{CIM/mnist-16-0-cmp-rm4-ci-max-1.png}\end{minipage} 
    \\ \hline
    \end{tabular}
    \caption{特徵圖對應法示意圖\cite{YangCNNInterpretable}}
    \label{tab:CIM_RMCI_example}
\end{table}

第二個方法為全連接層權重對應法，
是透過將全連接層的輸入$x$和全連接層的權重$w$的乘積視為$RM$
並且對應之$FM$為最後一個Block的$FM$，透過此$FM$找到$CI$
將全連接層轉成可以理解的資訊。

\section{文獻回顧}

本章節將向讀者介紹可解釋性人工智慧，並回顧可解釋性人中智慧的發展歷程和分類，
協助讀者可以更了解可解釋性人工智慧的現狀與研究方向，


\subsection{可解釋性人工智慧的演進分類}
可解釋性人工智慧(Explainable Artificial Intelligence, XAI)的研究最早可以追蹤到1991年的專家系統時代 W Swartout, C Paris 等人便發覺在金融、醫療，軍事等重要領域中進行的決策中往往需要一個足以支撐結論的合理解釋，
便從此開始對XAI進行研究 \cite{87686}。

XAI從一開始的小眾領域在近些年開始蓬勃發展成活躍的研究領域，
甚至每年都會有各式各樣的 Review Paper 被發表\cite{A2023100230} \cite{10188681}，
論其原因應該歸功於近年來人工智慧已被各行各業所應用於分類、物件偵測、資料分析等任務，
隨著人工智慧的普及，在重要領域中XAI的地位和重要性也日益上升。

根據 \cite{Nielsen_2022} 和 \cite{LONGO2024102301} 的研究，
我們將可解釋性人工智慧的研究分為 Ante-hoc 和 Post-hoc 兩大類，
然而這兩類方法均有各自的優缺點，
值得注意的是，在\cite{LONGO2024102301}有介紹到最近有些研究展現出具有解決這兩類缺點的潛力，
但其解釋性仍須更進一步的研究，因此我們將其視為不同於前兩類的第三大類。
以下讓我們介紹這些類別的特性與較為著名的研究。

\fig[1][fig:][H]{XAI.png}[可解人工智慧的分類(修改自\cite{YangCNNInterpretable})][可解釋人工智慧的分類]

\subsection{對於 Ante-hoc Explainable Model之研究}
Ante-hoc Explainable Model(\cite{Nielsen_2022}稱為 Interently Interpretable Models)，
不同於 Black Box 模型，在模型設計時就會考量可解釋性的需求，
使得模型本身便具備產生解釋性的能力，
其最終目標便是產生一個既準確又可以讓人理解決策過程的深度學習模型。

這類模型最著名的便是機器學習中的Decision Tree(DT)\cite{rokach2016decision}，
DT透過一系列的規則去不斷進行抉擇並且可以很容易地被可視化，
使得人們可以容易理解DT的決策邏輯，
並且也經常被用在金融等常運用表格去表示資料的領域\cite{grinsztajn2022tree}。
除了DT外也存在不同類型的模型，例如Nest Generalized Exemplar(NGE)\cite{salzberg1991nearest}、Adaptive-Network-Based Fuzzy Inference System(ANFIS)\cite{256541}、Collection of High Importance Random Path Snippets\\(CHIRPS)\cite{hatwell2020chirps}等等模型;

S. Salzberg 等人於 1991 提出了NGE\cite{salzberg1991nearest}的理論，
該理論將學習到的特徵映射到歐氏距離n維空間(簡稱為En)形成超矩形。
所謂的超矩形是一種幾何學的名詞，可以被視為是在n維空間中的長方體。
NGE便是利用超矩形由於是在n維空間，
因此可以互相與其他的超矩形進行疊加來形成任意深度的和超矩形每個軸可以是任意範圍的兩種特性，透過計算不同的輸入所形成的超矩形之間的距離，來得到高準確度，同時也可以透過幾何解釋模型作出決策的原因。然而這個方法在處理高維資料時面臨複雜性大幅提高的問題，這便是此模型侷限性。

J.-S.R. Jang 等人於1993提出ANFIS模型\cite{256541}，其將模糊系統與神經網路進行結合，由神經網路學習模糊規則並藉由模糊引擎來對輸入資料進行判斷。這個模型的本質上維持著模糊系統的架構，因此我們藉由導出模糊規則檢視模型的判斷條件來解釋模型作出的決策。

J. Hatwell等人於2020年提出CHIRPS演算法\cite{hatwell2020chirps}，該演算法是以Random Forest為基礎，從每棵樹中多數分類的決策路徑並從中找到最常出現的條件，
以這些條件建構出一條條if-then規則，並且分析每個規則的準確度和覆蓋率，提供如果缺少了該規則所造成的影響。該演算法提供之規則在當時與SOTA相同但其覆蓋率更大，並且演算法提供的是人類可以分析理解的規則而不是單純的結果。

以上的研究均在模型本身便具備可解釋性，然而這類模型存在一個問題便是必須在訓練模型前找出有效的特徵提取方法，
模型的可解釋性與準確度都會因為使用不同的特徵提取方法而造成很大的影響。
如何在使用不複雜的特徵提取的同時提高準確度與解釋性也是這類模型的很大的課題。

\subsection{對於 Post-hoc Explainable Method之研究}
Post-hoc Explainable Method 與 Ante-hoc Explainable Model最大的差異是，
此種研究所做的並不是開發與"black-box"模型(黑盒模型)相對具可解釋性的"white-box"模型，
而是著重於在一個模型完成預測之後，對其結果進行解釋。

根據\cite{Nielsen_2022}所述，在Post-hoc explainable之下可以分成兩類，Local 和 Global，
這兩類的差別在於Local的特點是針對單一筆預測進行解釋，通常採用的辦法是找出關鍵特徵並且解釋該特徵是如何影響到預測，Local之下針對方法又可以分成，以梯度為基礎的方法(Gradient-based)和以局部干擾為基礎的方法(Perturbation-based)，
Gradient-based最具代表性的便是Layer-wise Relevance Propagation(LRP)\cite{10.1007/978-3-319-44781-0_8}，
而Perturbation-based最具代表性則是Local Interpretable Model-agnostic Explanations(LIME)\cite{10.1145/2939672.2939778}。

A. Binder 等人於2016年提出了LRP演算法\cite{10.1007/978-3-319-44781-0_8}，該演算法是透過使用模型的倒傳遞演算法，將特定預測類別的分數逐層從輸出逐層傳遞，計算每層的相關性分數，最終回到輸入從而找出關鍵的輸入特徵。此種方法主要的優點便是除了神經網路外，它的原理可以被運用到不同的模型結構之中，但需要選擇合適的分數規則。而LRP的缺點缺點在於由於需要逐層計算每層的相關性分數所以複雜度較高。此外，由於其解釋性是依賴相關性分數的分配與計算，因此選擇合適的分數規則尤為重要。

M. T. Ribeiro 等人於2016年提出了LIME演算法\cite{10.1145/2939672.2939778}，該演算法的核心便是透過為想要解釋預測的輸入影像產生一組干擾樣本，這些干擾將會附加在原始輸入的局部部分形成干擾影像，之後將干擾影像輸入進黑盒模型中進行預測，在著訓練簡單模型(例如:DT、Linear Regression等)使這個簡單的模型能夠接近黑盒模型預測干擾影像的結果，最終透過解釋簡單模型來解釋原始輸入影像。這個演算法的優點在於可以靈活的使用在不同的資料類型和模型架構，然而其缺點在於無法提供模型整體的解釋並且可解釋性依賴於產生的干擾樣本的品質。

Global則是針對模型整體去進行解釋，總結出主要的特徵和判斷條件，呈現模型的判斷邏輯，其中最著名的代表則是SHapley Additive exPlanations(SHAP)\cite{NIPS2017_8a20a862}。

S. M. Lundberg 等人在2017年提出了SHAP這個演算法\cite{NIPS2017_8a20a862}。 SHAP運用了博弈論中Shapley Value的概念，SHAP會為
每個特徵計算其在不同特徵組合的平均貢獻來決定Shapley Value以表示特徵的重要性。此外，從研究中也表明每個類別只會有一組唯一的Shapley Value組合。這套方法的優點在於其可以同時提供全域和局部特徵的重要性並且其背後有堅實的博弈論的理論支撐。但其缺點在於實現和計算複雜度較高並且對於不了解博奕論的使用者會難以理解模型呈現出來的Shapley Value。

以上這些方法都是在"black-box"模型預測完後再對其進行解析，
然而這種方法也是存在一些缺點的，他們大多只是表示特定特徵的對於類別的重要性，
但是仍需要人類來解釋其背後所代表的意義，並且也無法解釋特徵是如何影響到類別。

\subsection{近年可解釋性模型趨勢之研究}
\cite{LONGO2024102301}中有寫到近年來也有一些論文展現出解決上面兩種方法缺點的潛力，
其中一個是將注意力的解釋整合到神經網路中，
透過順序注意力機制(Sequential Attention Mechanism)生成每個特徵在過程中的重要性分數來決定每個步驟中最重要的特徵，
以提高表格類資料的可解釋性的模型\cite{Arik_Pfister_2021}。
另一個是使用計算論證(Computational Arugmentation)的方式\cite{ijcai2021p600}，透過模擬人類推論的過程，
讓模型針對決策過程的每一步提出不同的規則相互論證，
由此來得到決策的每一步過程的正確規則。特別的是這類模型屬於非單調推理(Non-monotonic reasoning)，代表其允許有新證據時推翻自己的結論，這樣的方法較為符合人類的常見推理方式。
以上的方法雖然有望解決 Ante-Poc 和 Post-hoc explainable model的問題，但仍需更多的研究來驗證這一點。

\end{document}